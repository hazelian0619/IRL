IRL 设计思考报告（当前版本）
===========================

撰写目的
--------

本报告总结我们在 IRL（Inverse Reinforcement Learning）这一部分的全部讨论与当前设计决策，回答以下问题：

1. 在 60 天人格主线中，IRL 扮演什么角色？  
2. 我们如何从多模态情绪与行为出发，构造“状态 s_t”和“reward 代理 r_t”？  
3. 为什么不直接用 cluster+label，而要保留 IRL 视角？  
4. 我们采用的“两阶段 IRL + 模式解释”的混合策略是什么？  
5. 从横向（同行）与纵向（我们自己的主线）看，这条 IRL 设计是否合理，后续演化方向是什么？

本报告偏概念与架构层，不聚焦具体代码实现细节。

一、IRL 在 60 天人格主线中的角色
---------------------------------

主线目标不是“单纯预测情绪”，而是：

> 给定：  
> - 预设人格 + BFI 前测（pretest）的人物（如 Isabella）；  
> - 一条真实推进的 60 天 IRL 轨迹（文本/行为/情绪/心情分数）；  
> - BFI 后测（posttest），带有“经历这 60 天 IRL 之后”的语境；  
>  
> 需要回答：  
> - 这 60 天里，她**真正偏好**哪些情绪/行为模式？  
> - 这些偏好结构与她的 Big Five（前/后测）是否一致或发生轻微变化？  
> - 未来机器人/系统应该把她往什么样的状态推，才算“对她好”？

因此 IRL 的角色是：

- 不再停留在“她当前是什么情绪”，而是要给出一个**价值函数 R(s, a)**：  
  - 在什么状态 s 下、采取什么行为 a，Isabella 会觉得“好”（高 reward）或“糟”（低 reward）？  
  - 这个 value/reward 结构应与人格资料和产品目标对得上。

用一句话概括：

> 情绪模型解决“她现在在哪里”；  
> IRL 解决“哪些地方是她想反复回去的、哪些地方是她想远离的”，即“偏好结构”。

二、当前层级结构：从多模态情绪到 IRL 偏好
-------------------------------------------

我们已经搭出的层级与《12》的设计基本一致，可以简要归纳为：

### 2.1 人格层（trait-level）

- 预设人格：`preset_personality.json` 中的 Big Five 初始设定；  
- BFI 前测（pretest）：  
  - BFI-44 报告 + 计算得到的五维 0–1 分数；  
  - 与预设人格的相关性 r>0.75 作为“起点对齐”的质量门槛；  
- BFI 后测（posttest IRL）：  
  - IRL 结束后在“经历 60 天 IRL 之后”的语境下再施测；  
  - 得到 post Big Five 向量与 pre 的差值 Δ(O, C, E, A, N)。

### 2.2 情绪感知层（多模态融合）

输入：daily 多模态信号：

- 文本：nightly 对话内容；  
- 行为：日内工作/社交/休息/创作等行为摘要；  
- emotions JSON：LLM 对整日情绪的标签与解释；  
- mood_score：1–10 的日级自评或系统估计。

目标：得到一个“系统视角”的每日情绪概率分布：

- 为每个模态设计一个规则化/模型化的情绪预测器，输出 P_m(day)（4 类：积极/消极/中性/复杂）；  
- 以 emotions JSON 的 canonical label 作为初始 GT，计算各模态的预测精度；  
- 用精度 softmax 得到权重 w_text, w_behavior, w_emotion, w_score；  
- 通过 Late fusion 得到每日融合情绪概率：

> P_fusion(day) = Σ_m w_m · P_m(day)

这层输出的是：每天“积极/消极/中性/复杂”的统一概率分布 P_fusion(day)，  
代表系统在整合文本、行为、表情和自评分后的情绪判断。

### 2.3 状态层（时序情绪 embedding）

为什么不用单日作为状态？  
因为人格与偏好是慢变量，我们关心的是“模式”和“反应过程”：

- 持续几周高压 vs 偶尔一日心情差；  
- “高压→恢复”的动态；  
- 情绪波动是否长期偏高（潜在高 N）或偏平稳（潜在低 N）。

因此当前状态设计为**7 日滑窗的周级状态**：

- 60 天 → 7 日滑窗（stride=1）→ 54 个“7 日窗口”；  
- 每个窗口提取统计特征：  
  - valence（P(积极)-P(消极)) 的 mean/var/max/min/trend；  
  - social/work/rest/creative 行为的 7 日统计；  
  - 长期基线（通过 EWMA 得到）；  
- 使用 BiLSTM + 注意力对窗口序列编码，得到：

> z_t = 这一 7 日窗口在整个 60 天故事中的情绪/行为状态摘要。

这一层输出的状态序列：

> s_t ≡ z_t，t = 1..T（T≈54）

可理解为：“这周的生活整体是什么味道”，而不是单日快照。

### 2.4 reward 代理层（当前 V0）

为了给 IRL 提供“好坏”信号，我们目前采用了一个简单的 reward proxy：

- 对每日使用 valence(day) = P(积极) - P(消极) 表示“该日情绪好坏”；  
- 对每个 7 日窗口，取窗口内 valence 的平均：

> r_t = mean_{days in window t} valence(day)

含义：这一周整体偏积极（r_t > 0）还是偏消极（r_t < 0）。  
这是一个符合“情绪为核心”的 V0 设计，和心理学中用日/周情绪评分作为 outcome 的做法一致，但并不是唯一可能的 reward 定义。  
未来可以扩展为多维 reward（例如加入稳定性、目标进展等）。

### 2.5 IRL / 偏好层（当前 MVP）

当前 IRL MVP 实际上是一个 reward 建模原型：

- 输入：窗口级状态嵌入 z_t；  
- 目标：窗口级 reward 代理 r_t；  
- 使用一个 MLP 拟合 R(z_t) ≈ r_t，证明 z_t 中确实 encode 了 valence 型 reward 信息。

这一步不是“最终 IRL”，而是一个 **结构打桩（sanity check）**：

- 证明：在我们设计的状态空间上，确实能重构“这一周好不好”；  
- 为下一步“抽象偏好轴 f(s)+w”提供 evidence 和候选高 reward 片段。

### 2.6 人格对齐层（pre/post BFI）

这层已完成的工作：

- Isabella 的 BFI 前测 / 后测均已量化为五维 0–1 向量，并验证与预设人格的相关性（r≈0.91/0.89）；  
- 计算了 pre/post BFI 的差值 Δ(O,C,E,A,N)，并记录在自查文档中。  

下一步目标：

- 用 IRL 学出的偏好结构（后续的 f(s)+w）与 Big Five / persona 文本进行对齐与解释：  
  - w_social ↔ 外向性/宜人性；  
  - w_work ↔ 尽责性；  
  - w_high_variance ↔ 神经质；  
  - w_creative ↔ 开放性；  
- 并分析 pre/post BFI 的变化是否与高 reward 状态分布一致。

三、为什么不只用 cluster+label，而要保留 IRL 视角？
------------------------------------------------

一个自然的问题是：既然我们在 z_t 空间里做“高 reward 区域的发现”，  
这看起来和“聚类 + 打标签”非常像，为什么不直接用 cluster+label？

### 3.1 cluster+label 能做什么？

典型做法：

- 对 z_t 或日级特征做聚类，得到若干“轨迹模式”类别；  
- 回看每个 cluster 的原始时间序列，对其情绪/行为特征做描述：  
  - “持续低落型”；  
  - “高波动型”；  
  - “平稳中性型”等。

这样的优点：

- 能在无监督的情况下发现不同的情绪/行为模式；  
- 有助于心理学家理解“有哪些自然出现的状态类型”。

### 3.2 cluster 的两个局限：不看好坏，也不提供决策目标

1）**只看“像不像”，不看“好不好”**  

- 聚类基于相似度：在 feature 空间里近的放一起；  
- 但两个状态模式长得像，未必对 Isabella 来说都是“好状态”：
  - “持续忙碌+高 valence” 与 “持续忙碌+低 valence”，行为模式相似，但主观体验完全不同；
  - cluster 无法区分“她愿意重复的忙碌”和“她想逃离的忙碌”。

2）**没有“价值函数”，难以接到机器人策略上**  

- 聚类标签告诉你有模式 A/B/C，但：
  - 不知道哪一类是“应该多出现在生活中”的；  
  - 也不知道哪一类是“应该避免/缩短”的。  

IRL/reward 不同在于：

- 它显式定义了一个 R(s)：  
  - R(s) 高 = 值得追求的状态；  
  - R(s) 低 = 应该远离的状态；  
- 未来机器人/系统的策略可以用“maximize E[R(s)]”这样的目标来优化——这是 cluster 所缺失的能力。

### 3.3 我们的混合策略：IRL 做“价值排序”，cluster/人做“模式解释”

当前讨论形成的共识是：

> 不在 IRL 和 cluster 之间二选一，而是让 IRL 提供“好坏排序”，  
> 再用 cluster/人类分析来解释这些高/低 R(s) 区域对应的真实生活状态。

具体思路：

1. **第一阶段：在 z_t 空间用 IRL/reward 建模做 discovery**  

   - 用 MLP 在 z_t 上拟合 r_t（valence 型 reward）；  
   - 将 54 个周级状态按 R(z_t) 排序，找出高 reward 周、中等周、低 reward 周；  
   - 回看这些周对应的原始对话/行为/情绪曲线，观察：  
     - 高 reward 周有何共性？  
     - 低 reward 周有何共性？

   这一步类似于 **“reward-aware 的模式发现”**，区别于纯相似度聚类。

2. **第二阶段：把这些模式抽象成少数偏好轴 f(s) 与权重 w**

   - 从高/低 reward 的周级小故事中，总结出若干反复出现的 pattern：  
     - 例如：  
       - “有节奏的忙碌 + 有恢复 + 有社交/创作”；  
       - “长期高压 + 无休息”；  
       - “情绪高度稳定但偏平淡”；  
   - 将这些 pattern 固化为少量偏好特征 f_i(s)：  
     - f_social_balance, f_rest_presence, f_work_load, f_valence_stability, f_phase_xxx 等；  
   - 在 f(s) 空间上，用简单线性/MLP reward：R(s) = w·f(s)；  
   - w 的每一维都可解释，并可与 Big Five 和 persona 文本对齐。

这样：

- IRL 在 latent z 空间负责发现哪些状态“好/坏”；  
- cluster/人类分析负责读出这些高/低 R(s) 对应的具体生活模式；  
- f(s)、w 是在这些发现之上抽象出来的“偏好坐标系”，  
  最终成为可以讲人格 story 和约束机器人策略的结构。

四、横向：与同行做法的关系
-------------------------

从 IRL 与情绪/偏好建模的文献来看，我们的策略和几类典型方法有对应关系：

1. **经典 IRL（Abbeel & Ng, 2004 之后）**  
   - 手工设计 f(s)，reward 线性 R = w·f(s)，IRL 学 w；  
   - 优点：解释性强；缺点：f(s) 完全主观、许多假设在前。  
   - 我们的第二阶段（f(s)+w）与之相似，但 f(s) 的设计会参考第一阶段 IRL 在 z_t 空间发现的模式，以减少拍脑袋成分。

2. **深度 IRL / reward learning**  
   - 用深度网络从原始 state 学 latent 表示 + reward；  
   - 常在图像/复杂感知任务中使用，表达力强，但 reward 和 state 都成黑盒；  
   - 对我们的场景（单 persona、60 天）来说，容易 overfit，且难与人格测量对齐。

3. **情绪/心理健康中的 HMM/cluster + label**  
   - 手段：对情绪/行为时间序列做无监督聚类，  
     然后由心理学家回看时间序列与生活事件，对各类 pattern 起名；  
   - 这种“两阶段：模式发现 → 专家解释”的做法，与我们“IRL latent → 模式解释 → f(s)”非常相似；  
   - 不同之处在于，我们多了一层 reward 的 notion：不是所有模式都是“高价值”的。

4. **推荐系统 / 用户 embedding + persona 解释**  
   - 先学 latent factors，再通过对高/低因子用户的行为分析，为这些因子起名；  
   - 本质是“模型 discovery +人类解释”，和我们的思路有明显亲缘。

综上：当前 IRL 设计在横向上可以被视为：

> 经典线性 IRL（f(s)+w）的可解释性目标  
> + 深度 IRL/latent reward 的模式发现能力  
> + 情绪/心理学领域 cluster+label 的解释流程  
> 的一个折中方案。

五、纵向：放在我们自己的 60 天人格主线上
------------------------------------

纵向层级上，我们的 pipeline 是：

1. 日级多模态：对话/行为/情绪/分数；  
2. 日级情绪融合：P_fusion(day)；  
3. 周滑窗：以 7 日窗口表达“这段时间”的情绪/行为模式；  
4. BiLSTM+注意力：理解这些状态在 60 天故事中的上下文位置，得到 z_t；  
5. IRL/reward：用 r_t 对 z_t 进行“好坏排序”；  
6. 模式解释：从高/低 R(s) 周中读出候选偏好模式；  
7. f(s)+w：固化成少量偏好轴并与人格对齐；  
8. 前/后 BFI：检验与 trait-level 的一致性/变化。

在这条线上：

- 日级层：适合用来画图、看事件、做细粒度解释；  
- 周级层：适合放 IRL，因为偏好和情绪调节是慢变量；  
- IRL 层不再尝试从日级 raw data 中直接学奖励（容易变成 noise fitting），  
  而是在一个已经有心理学动机的周级状态空间中，做 reward-aware 的偏好发现与解释。

六、当前 IRL 设计的定位与下一步
------------------------------

当前状态：

- 状态 s_t：已通过 7 日滑窗 + 滑动统计 + BiLSTM+注意力得到 z_t；  
- reward 代理 r_t：基于 fusion_valence 的窗口平均，作为 V0 “这一周整体好坏”的信号；  
- IRL MVP：使用 MLP 在 z_t 上拟合 r_t，发现状态 embedding 中确实有足够信息 encode valence 型 reward；  
- BFI 前/后测：已量化为 Big Five 向量，验证与预设人格一致，并算出 Δ(O,C,E,A,N)。

下一步关键工作（IRL 视角）：

1. 使用当前 IRL MVP 在 z_t 空间进行 reward-aware 模式挖掘：  
   - 找出高 reward 的周、中等和低 reward 的周；  
   - 回看这些周的原始对话/行为/情绪序列，提炼候选偏好模式。

2. 在这些候选模式基础上，抽象出少量偏好特征 f(s)：  
   - 行为结构（social/work/rest/creative 的周级组合）；  
   - 情绪水平与稳定性（valence 均值/方差/趋势/高压天数）；  
   - 故事阶段（phase embedding）；  
   - 特征维度数量控制在 5–10 以内，以便解释与对齐。

3. 在 f(s) 空间上构建一个简单可解释的 reward 结构 R(s)=w·f(s)：  
   - 学习 w 使其拟合 r_t（或者更丰富的 reward proxy）；  
   - 用 BFI 前/后测和 persona 文本解释 w 的符号与大小。

最终目标不是“证明 IRL 算法最先进”，而是：

> 构建一个心理学与工程上都合理的结构：  
> 情绪感知 → 状态表示 → 偏好结构 → 人格对齐与策略目标，  
> 让机器人/系统能围绕“这种情绪/行为模式对她是好事”的价值函数来做长期决策。

