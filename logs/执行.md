# IRL V2 执行计划（Isabella 60d）

  ## 1. 项目目标与范围

  核心目标（当前阶段）：

  - 在已有的 60 天多模态情绪轨迹上，构建一个 周级偏好 IRL 模型：
      - 输入：每周的可解释特征 φ(s_t)；
      - 输出：一个线性偏好函数 R_θ(s) = θᵀφ(s)，能把“明显好的周”和“明显差的周”区分开；
      - θ 的每一维有清晰含义，可以和人格量表（BFI）以及故事线对齐。
  - 保持 工业级可执行：
      - 数据流清晰、脚本可复现、参数/路径可配置；
      - LLM 融合层是稳定的、无规则 fallback 的。

  暂不做（明确 out-of-scope）：

  - 不做在线 IRL 更新（online policy learning / interactive feedback）；
  - 不做完整 MDP / MaxEnt IRL（没有显式 action 和环境）；
  - 不追求 valence/reward 的 “SOTA 数字”，重点是结构与解释力。

  ———

  ## 2. 当前进度对齐（已完成 / 基线）

  2.1 多模态情绪融合（已完成 V1，使用 LLM）

  - 每日多模态：
      - 文本：对话 transcript_md → LLM → P_text(day) ∈ R⁴；
      - 行为：behaviors.json 汇总描述 → LLM → P_behavior(day) ∈ R⁴；
      - emotions JSON：label → one-hot P_emotion(day)；
      - mood_score：按区间映射成 one-hot P_score(day)。
  - 融合：
      - 用 60 天上各模态对 canonical(emotions.label) 的 accuracy；
      - softmax(acc) 得到 w_text, w_beh, w_emotion, w_score；
      - P_fusion(day) = ∑ w_mod · P_mod(day) → fusion_daily.npy。
  - valence：
      - valence(day) = P_fusion(积极) - P_fusion(消极)。

  2.2 周级状态与 reward 代理（已完成）

  - 7 日滑窗：
      - window_valence.npy（r_t：这一周 valence 均值）；
      - rolling_stats.npy（[mean,var,max,min,slope]）；
  - BiLSTM+Attention 状态 encoder：
      - temporal_embeddings.npy：z_t（当前主要用作 IRL MVP 的 sanity check）；
  - IRL V0：
      - run_irl_mvp_isabella.py 训练 MLP：R(z_t) ≈ r_t，仅用于验证 z_t 里有 reward 信息。

  结论：

  - 第一层（多模态融合）和第二层（周级状态 + r_t）已经成型；
  - 第三层 IRL 目前只有 “回归 MVP”，尚未体现 IRL 的偏好结构优势。

  ———

  ## 3. 设计原则：工业标准 vs 算法精髓

  工业侧：

  - 重用预训练的 LLM 作为文本/行为 encoder，不自行发明复杂网络；
  - 所有 pipeline 通过脚本运行，输入 --root；输出固定到 features/ 或 models/；
  - 结果以 .npy + .json/.md 形式保存，便于检查和复用。

  算法侧（我们这层的“精髓”）：

  - 不再直接拟合 r_t，而是：
      - 把 r_t 转为“周与周之间的偏好对 (i ≻ j)”；
      - 在线性可解释特征空间 φ(s_t) 上，学习 θ 使这些偏好被最大化满足。
  - θ 的物理意义清晰：
      - 每一维是一个“情绪生活维度”的偏好权重；
      - 可以和 BFI/persona 文本做一一映射。

  ———

  ## 4. 执行路线图（分阶段）

  ### Phase A：巩固融合 & valence（状态：已基本完成）

  目标：
  确认 LLM 版多模态融合和 valence 输出在两个数据集上稳定可用，为后面 IRL 提供干净输入。

  输入：

  - data/isabella_irl_60d_openai_v2
  - data/isabella_irl_3d_clean

  工作要点：

  - 确保：
      - IRL_FUSION_USE_LLM_BACKEND=true；
      - 无任何规则 fallback（LLM 出错直接 raise，不 silently 改成规则版）。
  - 对两个 root：
      - 运行 scripts/export_fusion_features.py --root ...；
      - 检查输出：
          - fusion_daily.npy (T,4)；
          - fusion_meta.json（保存 days、模态权重等）。

  验收标准：

  - 两个数据集都成功导出融合结果，per-modality accuracy & weights 可解释；
  - 没有规则版逻辑被调用。

  > 状态：已做到，可以视为 Phase A 完成。

  ———

  ### Phase B：周级可解释特征 φ(s_t)

  目标：
  在已有 window_valence + 行为特征基础上，定义一组小而精的周级特征 φ(s_t)，每一维都有清晰含义，为 IRL 提供解释性空间。

  建议特征集（示意，可 5–10 维）：

  1. valence_mean：该周 valence 平均（= r_t）
  2. valence_var：该周 valence 方差
  3. valence_min：该周最差一天的 valence
  4. valence_trend：该周 valence 线性拟合斜率（向上/向下）
  5. social_level：该周社交强度（从行为统计聚合）
  6. rest_level：该周休息/低强度日比例
  7. work_level：该周工作天数或强度
  8. conflict_level：该周冲突/情绪劳动的 proxy
  9. phase_election：是否处于选举期
  10. phase_valentine / phase_recovery：故事阶段 indicator（可选）

  实现：

  - 新增模块：companion-robot-irl/features/preference_features.py
      - 函数示意：

        def export_preference_features(root: str) -> None:
            """
            从 <root> 下已有的 features + behaviors/emotions 等，
            计算每个周 t 的 φ(s_t)，保存到：
                <root>/features/preference_features.npy  (T_week, F)
                <root>/features/preference_feature_names.json
            """
      - 数据源：
          - window_valence.npy, rolling_stats.npy；
          - features/X_weekly_behavior.npy（如已有，否则可以在这里简单聚合 behaviors）；
          - temporal_meta.json 或 days mapping，用于对齐天→周；
          - 一个简单的 “phase mapping”（按 day index 划分 election/valentine 等）。

  脚本：

  - scripts/export_preference_features.py：

    cd companion-robot-irl
    python3 scripts/export_preference_features.py --root data/isabella_irl_60d_openai_v2

  输出：

  - preference_features.npy (T_week, F)
  - preference_feature_names.json（每一维的名字和说明）

  验收标准：

  - F 在 5–10 之间，每一维都能用一句人话解释；
  - 打印几行样本（某些周的 φ）能看出明显差异（例如选举周 vs 恢复周）。

  ———

  ### Phase C：从 r_t 构造偏好对 (i ≻ j)

  目标：
  把周级 reward 代理 r_t（window_valence）转换成 IRL 可用的偏好约束集。

  实现：

  - 新增模块：companion-robot-irl/learning/irl_preference.py
      - 函数 1：构造偏好对

        def build_preference_pairs(
            r: np.ndarray,
            top_k: int = 5,
            bottom_k: int = 5,
            margin: float = 0.1,
        ) -> list[tuple[int, int]]:
            """
            r: shape (T_week,), window_valence 序列
            返回若干 (i, j)，表示 i ≻ j
            """

        逻辑：
          - 排序 54 个 r_t；
          - 取 top_k 个 index 作为 Top，bottom_k 作为 Bottom；
          - 构造所有 (i ∈ Top, j ∈ Bottom)；
          - 可选：对所有 i,j，如果 r[i] - r[j] > margin，再加入一次偏好对。
      - 函数 2：调试/可视化

        def summarize_pairs(days_for_week, r, pairs):
            """
            打印每个偏好对对应的 weeks 和 day 范围，以及 r 值，
            帮助人眼检查是否符合 story。
            """

  脚本：

  - scripts/inspect_irl_preference_pairs.py：
      - 读 window_valence.npy；
      - 调用 build_preference_pairs；
      - 打印 top/bottom weeks 的 day 范围、r_t、以及对应阶段（valentine/election 等）。

  输出：

  - irl_preference_pairs.json（保存 (i,j) 列表，方便复用）
  - 控制台上的 summary（人眼校验）

  验收标准：

  - 选出的 Top weeks 和 Bottom weeks，与你对 Isabella 的故事理解一致（比如情人节准备周 vs 选举高压周）；
  - 偏好对数量控制在几十级别（比如 20–50），不是上千个，避免过拟合噪声。

  ———

  ### Phase D：偏好 IRL 训练 θ

  目标：
  在 φ(s_t) 空间上，用上述偏好对 (i ≻ j) 训练一个线性 reward 模型 R_θ(s) = θᵀφ(s)。

  实现：

  - 在 learning/irl_preference.py 中继续：

    def fit_preference_reward(
        phi: np.ndarray,  # (T_week, F)
        pairs: list[tuple[int, int]],
        l2_reg: float = 1e-3,
        lr: float = 1e-2,
        num_steps: int = 500,
    ) -> np.ndarray:
        """
        用 logistic preference loss 拟合 θ：
            L(θ) = Σ log σ(θᵀ(φ_i - φ_j)) - λ||θ||²
        返回 θ: (F,)
        """
      - 可以用纯 numpy / scipy 做小规模梯度下降；
      - 或者用 sklearn 的 LogisticRegression 把 (φ_i - φ_j) 当作输入、label=1 做二分类；
      - 关键是满足我们的 “pairwise preference” 目标，而不是回归 r_t。
  - 训练脚本：scripts/run_irl_preference_isabella.py

    流程：
      1. 加载：
          - preference_features.npy → φ；
          - window_valence.npy → r；
          - irl_preference_pairs.json（或现场构造）；
      2. 调用 fit_preference_reward，得到 θ；
      3. 计算每个 week 的 R_θ(s_t)；
      4. 保存：
          - features/irl_theta.npy；
          - features/irl_reward_weekly.npy（R_θ(s_t) 序列）；
      5. 打印：
          - θ 每一维的数值 + 对应特征名；
          - 在偏好对上的 “预测正确率”，比如 P(R(i) > R(j)) 的百分比。

  验收标准：

  - 在大多数偏好对 (i ≻ j) 上，R_θ(s_i) > R_θ(s_j)（正确率显著高于 50%，比如 >80%）；
  - θ 的符号和大小符合直觉（比如 “选举 phase 权重是负的”，“rest 权重为正”）。

  ———

  ### Phase E：人格对齐 & 报告

  目标：
  把 θ 解释成“人格化的偏好向量”，并与 BFI/persona 文本对齐，形成可以直接写进论文/报告的内容。

  步骤：

  1. 生成一张特征-权重表：
      - 从 preference_feature_names.json + irl_theta.npy；
      - 生成表格（可放入 logs/irl_preference_theta_isabella.md）：

        | feature          | 含义                         | θ_i  | 解释（IRL 偏好）           |
  2. 对照 BFI/persona：
      - 把 BFI 前测 / 后测结果简单列出；
      - 对每类人格维度（E/A/C/O/N）给出预期的 θ pattern；
      - 看实测 θ 是否与预期大体一致，写出解释文字。
  3. 对比监督回归（可选，但推荐）：
      - 用同样的 φ(s_t)，做一个线性回归：w_reg 拟合 r_t；
      - 对比 w_reg vs θ：
          - 是否 IRL 更强调“极端周的分离”，而不是贴 r_t；
          - 是否在 “conflict/rest/social”等维度上有更明显的偏好结构。
  4. 写一段“方法 + 结果 + 讨论”：
      - 方法：偏好对构造 + 线性偏好 IRL；
      - 结果：θ pattern + 偏好对准确率；
      - 讨论：和 BFI 的一致性、和回归 baseline 的差异。

  验收标准：

  - 你自己读完 θ 表和解释段，主观感觉：
      - “这就是我对 Isabella 的理解，用数学语言表达出来了”；
  - 这段文字可以直接迁到 design report 或论文 skeleton 里。

  ———

  ## 5. 后续扩展（数据补充后）

  1. 多轴偏好：
      - 把 θ 拆成多个头：valence/stability/social/rest/成长 等；
      - 对不同类型的偏好对单独训练或多任务训练；
      - 得到一个偏好向量 V(s_t) 而不是单标量。
  2. 多 persona / 多 run：
      - 在多个 60d 轨迹上训练 θ_global；
      - 对每个 persona 做小幅 finetune 得到 θ_user；
      - 研究 “人格 → 偏好向量 θ 的映射” 是否有稳定 pattern。
  3. 轨迹级 IRL：
      - 用整条轨迹的累积 R_θ(τ) 和打乱/拼接的负例轨迹；
      - 用 pairwise trajectory loss（τ_E ≻ τ_neg）学习更宏观的 reward。
  4. 在线 IRL（未来阶段）：
      - 在真实机器人交互中，从轻量反馈构造新偏好对；
      - 小步在线更新 θ_user。

  ———

  ## 6. 自检：这份计划是否平衡了“工程化”和“IRL 精髓”？

  工程侧检查：

  - 每一层都有明确的输入/输出文件格式；
  - 所有关键步骤通过脚本运行，可以在 CI 或本地反复重跑；
  - LLM 融合层已经去掉规则 fallback，逻辑清晰可控；
  - IRL 部分只引入轻量的线性模型与 logistic loss，在你目前数据量下是可训练的。

  算法/新颖性侧检查：

  - 不再是简单的 R(z_t) ≈ r_t 回归：
      - 核心是从 r_t → (i ≻ j) 的偏好转换；
      - 在可解释的 φ(s) 空间上学 θ，使偏好约束最大化满足。
  - θ 的解释和 BFI/persona 对齐，是这套 IRL 的“灵魂”：
      - 你能用它讲清楚“她到底偏好什么样的周”，而不是只给一个分数。
  - 保留了未来扩展到多轴偏好 / 多 persona / 轨迹级 IRL 的空间，不会把架构锁死。
